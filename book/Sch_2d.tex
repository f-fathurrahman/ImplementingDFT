\chapter{Schroedinger equation in 2d}

Now we will turn our attention to higher dimensions, i.e 2d.
The time-independent Schrodinger equation in 2d can be written as:
\begin{equation}
\left[ -\frac{1}{2}\nabla^2 + V(x,y) \right] \psi(x,y) = E\,\psi(x,y)
\label{eq:sch_2d}
\end{equation}
%
where $\nabla^2$ is the Laplacian operator:
%
\begin{equation}
\nabla^2 = \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2}
\end{equation}



\section{Describing grid in 2d}

Now we have two directions $x$ and $y$. Our approach to solving the Schroedinger equation
is similar to the one we have used before in 1d, however several technical difficulties
will arise.

To describe the computational grid, we now need to specify
$x_{\mathrm{max}}, x_{\mathrm{min}}$ for the $x$-domain
and $y_{\mathrm{max}}, y_{\mathrm{min}}$ for $y$-domain. We also need to specify number of grid
points in for each x and y-directions, i.e. $N_{x}$ and $N_{y}$.
There are quite lot of variables.
For easier management, we will collect our grid related variables in one data structure or
\txtinline{struct} in Julia. A \txtinline{struct} in Julia looks very much like C-struct.
It also defines a new custom data type in Julia.

Our struct definition looks like this.
\begin{juliacode}
struct FD2dGrid
  Npoints::Int64
  Lx::Float64
  Ly::Float64
  Nx::Int64
  Ny::Int64
  hx::Float64
  hy::Float64
  dA::Float64
  x::Array{Float64,1}
  y::Array{Float64,1}
  r::Array{Float64,2}
  idx_ip2xy::Array{Int64,2}
  idx_xy2ip::Array{Int64,2}
  pbc::Tuple{Bool,Bool}
end
\end{juliacode}
%
An instance of \txtinline{FD2dGrid} can be initialized using the following constructor function:
%
\begin{juliacode}
function FD2dGrid( x_domain, Nx, y_domain, Ny )
  x, hx = init_FD1d_grid(x_domain, Nx)
  y, hy = init_FD1d_grid(y_domain, Ny)
  dA = hx*hy
  Npoints = Nx*Ny
  r = zeros(2,Npoints)
  ip = 0
  idx_ip2xy = zeros(Int64,2,Npoints)
  idx_xy2ip = zeros(Int64,Nx,Ny)
  for j in 1:Ny
    for i in 1:Nx
      ip = ip + 1
      r[1,ip] = x[i]
      r[2,ip] = y[j]
      idx_ip2xy[1,ip] = i
      idx_ip2xy[2,ip] = j
      idx_xy2ip[i,j] = ip
    end
  end
  return FD2dGrid(Npoints, Nx, Ny, hx, hy, dA, x, y, r, idx_ip2xy, idx_xy2ip) 
end
\end{juliacode}

A short explanation about the members of \txtinline{FD2dGrid} follows.
%
\begin{itemize}
%
\item \txtinline{Npoints} is the total number of grid points.
%
\item \txtinline{Nx} and \txtinline{Ny} is the total number of grid points
in $x$ and $y$-directions, respectively.
%
\item \txtinline{hx} and \txtinline{hy} is grid spacing in $x$ and $y$-directions,
respectively. \txtinline{dA} is the product of \txtinline{hx} and \txtinline{hy}.
%
\item \txtinline{x} and \txtinline{y} are the grid points in $x$ and $y$-directions.
The actual two dimensional grid points $r \equiv (x_{i},y_{i})$ are stored as
two dimensional array $r$.
%
\item Thw two integers arrays \txtinline{idx_ip2xy} and \txtinline{idx_xy2ip} defines
mapping between two dimensional grids and linear grids.
\end{itemize}


As an illustration let's build a grid for a rectangular domain
$x_\mathrm{min} = y_{\mathrm{min}}=-5$ and $x_\mathrm{max} = y_{\mathrm{max}}=5$
and $N_{x}=3$, $N_{y}=4$.
Using the above constructor for \txtinline{FD2dGrid}:
\begin{juliacode}
Nx = 3
Ny = 4
grid = FD2dGrid( (-5.0,5.0), Nx, (-5.0,5.0), Ny )
\end{juliacode}
%
Dividing the $x$ and $y$ accordingly we obtain $N_{x}=3$
grid points along $x$-direction
%
\begin{textcode}
julia> println(grid.x)
[-5.0, 0.0, 5.0]
\end{textcode}
%
and $N_{y}=4$ points along the $y$-direction
\begin{textcode}
julia> println(grid.y)
[-5.0, -1.6666666666666665, 1.666666666666667, 5.0]
\end{textcode}
%
The actual grid points are stored in \txtinline{grid.r}. Using the
following snippet, we can printout all of the grid points:
%
\begin{juliacode}
for ip = 1:grid.Npoints
    @printf("%3d %8.3f %8.3f\n", ip, grid.r[1,ip], grid.r[2,ip])
end
\end{juliacode}
%
The results are:
%
\begin{textcode}
  1   -5.000   -5.000
  2    0.000   -5.000
  3    5.000   -5.000
  4   -5.000   -1.667
  5    0.000   -1.667
  6    5.000   -1.667
  7   -5.000    1.667
  8    0.000    1.667
  9    5.000    1.667
 10   -5.000    5.000
 11    0.000    5.000
 12    5.000    5.000
\end{textcode}
%
We also can use the usual rearrange these points in the usual 2d grid rearrangement:
%
\begin{textcode}
[  -5.000,  -5.000] [  -5.000,  -1.667] [  -5.000,   1.667] [  -5.000,   5.000] 
[   0.000,  -5.000] [   0.000,  -1.667] [   0.000,   1.667] [   0.000,   5.000] 
[   5.000,  -5.000] [   5.000,  -1.667] [   5.000,   1.667] [   5.000,   5.000]
\end{textcode}
%
which can be produced from the following snippet:
%
\begin{juliacode}
for i = 1:Nx
    for j = 1:Ny
        ip = grid.idx_xy2ip[i,j]
        @printf("[%8.3f,%8.3f] ", grid.r[1,ip], grid.r[2,ip])
    end
    @printf("\n")
end
\end{juliacode}



\section{Laplacian operator}

Having built out 2d grid, we now turn our attention to the second derivative operator or
the Laplacian in the equation \ref{eq:sch_2d}.
There are several ways to build a matrix representation of the Laplacian, but we will
use the easiest one. 

Before constructing the Laplacian matrix, there is an important observation that
we should make about the second derivative matrix $\mathbb{D}^{(2)}$. We should note
that the second derivative matrix contains mostly zeros. This type of matrix that
most of its elements are zeros is called \textbf{sparse matrix}.
In a sparse matrix data structure, we only store its non-zero elements with specific
formats such as compressed sparse row/column format (CSR/CSC) and coordinate format.
We have not made use of the sparsity of the second derivative matrix
in the 1d case for simplicity. In the higher dimensions, however,
we must make use of this sparsity, otherwise we will waste computational resources 
by storing many zeros. The Laplacian matrix that we will build from
$\mathbb{D}^{(2)}$ is also very sparse.

Given second derivative matrix in $x$, $\mathbb{D}^{(2)}_{x}$,
$y$ direction, $\mathbb{D}^{(2)}_{x}$,
we can construct finite difference representation of the Laplacian operator
$\mathbb{L}$ by using
%
\begin{equation}
\mathbb{L} = \mathbb{D}^{(2)}_{x} \otimes \mathbb{I}_{y} +
\mathbb{I}_{x} \otimes \mathbb{D}^{(2)}_{y}
\end{equation}
%
where $\otimes$ is Kronecker product.
In Julia, we can use the function \jlinline{kron} to form the Kronecker product
between two matrices \jlinline{A} and \jlinline{B} as \jlinline{kron(A,B)}.

The following function illustrates the above approach to construct matrix
representation of the Laplacian operator.
\begin{juliacode}
function build_nabla2_matrix( grid::FD2dGrid )
  Nx = grid.Nx; hx = grid.hx
  Ny = grid.Ny; hy = grid.hy
  D2x = build_D2_matrix_9pt(Nx, hx)
  D2y = build_D2_matrix_9pt(Ny, hy)
  ∇2 = kron(D2x, speye(Ny)) + kron(speye(Nx), D2y)
  return ∇2
end
\end{juliacode}

The standard Julia library does not include definition for \jlinline{speye} function
but it can be implemented by the following definition.
\begin{juliacode}
speye(N::Int64) = sparse( Matrix(1.0I, N, N) )
\end{juliacode}

In the Figure \ref{fig:fd_gaussian_2d}, an example to the approximation of 2nd derivative
of 2d Gaussian function by using finite difference is shown.

\begin{figure}[h]
{\center
\includegraphics[width=0.45\textwidth]{../codes/FD2d/IMG_gaussian2d.pdf}\,%
\includegraphics[width=0.45\textwidth]{../codes/FD2d/IMG_d2_gaussian2d.pdf}
\par}
\caption{Two-dimensional Gaussian function and its finite difference
approximation of second derivative}
\label{fig:fd_gaussian_2d}
\end{figure}

The following program is used to produce the figure.
\begin{juliacode}
using Printf
using LinearAlgebra
using SparseArrays
    
import PyPlot
const plt = PyPlot
    
include("FD2dGrid.jl")
include("build_nabla2_matrix.jl")
include("supporting_functions.jl")
    
function my_gaussian( grid; α=1.0 )
  Npoints = grid.Npoints
  f = zeros(Npoints)
  for i in 1:Npoints
    x = grid.r[1,i]
    y = grid.r[2,i]
    r2 = x^2 + y^2
    f[i] = exp(-α*r2)
  end
  return f
end
    
function main()  
  Nx = 75
  Ny = 75
  grid = FD2dGrid( (-5.0,5.0), Nx, (-5.0,5.0), Ny )
    
  ∇2 = build_nabla2_matrix( grid )
    
  fg = my_gaussian(grid, α=0.5)
  plt.clf()
  plt.surf(grid.x, grid.y, reshape(fg, grid.Nx, grid.Ny), cmap=:jet)
  plt.gca(projection="3d").view_init(30,7)
  fileplot = "IMG_gaussian2d.pdf"
  plt.savefig(fileplot)
    
  d2fg = ∇2*fg    
  plt.clf()
  plt.surf(grid.x, grid.y, reshape(d2fg, grid.Nx, grid.Ny), cmap=:jet)
  plt.gca(projection="3d").view_init(30,7)    
  fileplot = "IMG_d2_gaussian2d.pdf"
  plt.savefig(fileplot)
end
    
main()    
\end{juliacode}


\section{More about sparse matrices in Julia}

Using the function \jlinline{typeof} we can know the type of the variable
\jlinline{∇2} which was returned by the function \jlinline{build_nabla2_matrix}:
\begin{textcode}
julia> typeof(∇2)
SparseMatrixCSC{Float64,Int64}  
\end{textcode}
The \jlinline{CSC} part here is actually describe a certain format to store
a sparse matrix. It stands for \textbf{C}ompressed \textbf{S}parse \textbf{C}olumn
format. There are other formats as well: such as Compressed Sparse Row format (CSR),
coordinate list format (COO), and several others.

The CSC format stores a sparse matrix $\mathbf{A}$ with size $m \times n$
using three (one-dimensional) arrays (nzval, colptr, rowval).
Let Nnz denote the number of nonzero entries in $\mathbf{A}$.

The arrays nzval and rowval are of length Nnz, and contain the non-zero values
and the row indices of those values respectively.

An integer array colptr contains pointers to the beginning of each column in the
arrays nzval and rowval. Thus the content of colptr[i] is the position in
arrays nzval and rowval where the i-th row starts.
The length of colptr is n + 1 with colptr[n + 1] containing
the number colptr[1] + Nnz, i.e., the address in
nzval and rowval of the beginning of a
fictitious column m + 1.

The array colptr has one element per column in the matrix and encodes
the index in nzval where the given column starts. 
It may contain an extra end element which is set to Nnz.

Field names:
\begin{textcode}
julia> fieldnames(typeof(∇2))
(:m, :n, :colptr, :rowval, :nzval)  
\end{textcode}

From the documentation
\begin{juliacode}
struct SparseMatrixCSC{Tv,Ti<:Integer} <: AbstractSparseMatrix{Tv,Ti}
  m::Int                  # Number of rows
  n::Int                  # Number of columns
  colptr::Vector{Ti}      # Column j is in colptr[j]:(colptr[j+1]-1)
  rowval::Vector{Ti}      # Row indices of stored values
  nzval::Vector{Tv}       # Stored values, typically nonzeros
end
\end{juliacode}

Example 1:
\begin{equation}
A = \begin{bmatrix}
0  &  0  &  1.0 &  0.0 \\
5  &  8  &  0.0 &  0.0 \\
0  &  0  &  3.0 &  0.0 \\
0  &  6  &  0.0 &  0.0 \\
0  &  0  &  0.0 &  0.0
\end{bmatrix}
\end{equation}

\begin{textcode}
nzval = [5.0, 8.0, 6.0, 1.0, 3.0]
rowval = [2, 2, 4, 1, 3]
colptr = [1, 2, 4, 6, 6]
\end{textcode}

Example 2:
\begin{equation}
\mathbf{B} = \begin{bmatrix}
0  &  0  &  1  &  5 \\
5  &  0  &  0  &  0 \\
0  &  0  &  3  &  0 \\
0  &  0  &  0  &  0 \\
0  &  0  &  1  &  0 \\
5  &  0  &  0  &  6
\end{bmatrix}
\end{equation}

\begin{textcode}
nzval  = [5.0, 5.0, 1.0, 3.0, 1.0, 5.0, 6.0]
rowval = [2, 6, 1, 3, 5, 1, 6]
colptr = [1, 3, 3, 6, 8]
\end{textcode}

Example 3:
\begin{equation}
\mathbf{C} = \begin{bmatrix}
 -2  &  1  &  0  &  0  &  0 \\
  1  & -2  &  1  &  0  &  0 \\
  0  &  1  & -2  &  1  &  0 \\
  0  &  0  &  1  & -2  &  1 \\
  0  &  0  &  0  &  1  & -2
\end{bmatrix}
\end{equation}

\begin{textcode}
nzval  = [-2.0, 1.0, 1.0, -2.0, 1.0, 1.0, -2.0, 1.0, 1.0, -2.0, 1.0, 1.0, -2.0]
rowval = [1, 2, 1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5]
colptr = [1, 3, 6, 9, 12, 14]
\end{textcode}

Another way:
sparse(I,J,V) then constructs a sparse matrix such that S[I[k], J[k]] = V[k].

\section{Iterative methods for eigenvalue problem}

Now that we know how to build the Laplacian matrix, we now can build the Hamiltonian
matrix given some potential:
\begin{juliacode}
∇2 = build_nabla2_matrix( grid )
Ham = -0.5*∇2 + spdiagm( 0 => Vpot )
\end{juliacode}
Note that we have used sparse diagonal matrix for building the potential matrix by
using the function \txtinline{spdiagm}.
Our next task after building the Hamiltonian matrix is to find the eigenvalues
and eigenfunctions.
However, note that the Hamiltonian matrix size is large.
For example, if we use $N_x=50$ and $N_y=50$ we will end up with a Hamiltonian
matrix with the size of $2500$.
The use of \txtinline{eigen} method, as we have done in the 1d case,
to solve this eigenvalue problem is thus not practical.
Actually, given enough computer memory and time, we can use the function
\txtinline{eigen} anyway to find all eigenvalue and eigenfunction pairs of
the Hamiltonian, however it is not recommended
nor practical for larger problem size.

Typically, we do not need to solve for all eigenvalue and eigenfunction pairs.
We only need to solve for several eigenpairs with lowest eigenvalues. In a typical density
functional theory calculations, we only need to solve for $N_{\mathrm{electrons}}$ or
$N_{\mathrm{electrons}}/2$ lowest states, where $N_{\mathrm{electrons}}$ is the number
of electrons in the system.

In numerical methods, there are several methods to search for several eigenpairs
of a matrix. These methods falls into the category of \textit{partial or iterative
diagonalization methods}. Several known methods are Lanczos method, Davidson method,
preconditioned conjugate gradients, etc.
More detailed discussion about these methods are deferred to Appendix XXX.
We have prepared several implementation of iterative diagonalization methods
which can be used for black boxes for your convenience:
%
\begin{itemize}
\item \txtinline{diag_Emin_PCG}
\item \txtinline{diag_davidson}
\item \txtinline{diag_LOBPCG}
\end{itemize}
%
These functions have similar function signatures. They typically
need Hamiltonian, initial guess of wave function and preconditioner.
An example of \jlinline{diag_LOBPCG} is given below.
%
\begin{juliacode}
function diag_LOBPCG!( Ham, X::Array{Float64,2}, prec;
                       tol=1e-5, NiterMax=100, verbose=false,
                       verbose_last=false, Nstates_conv=0 )
\end{juliacode}
%
The three mandatory arguments are as follow:
%
\begin{itemize}
\item \jlinline{Ham}: the Hamiltonian matrix
\item \jlinline{X::Array{Float64,2}}: initial guess of eigenfunctions
\item \jlinline{prec}: the preconditioner
\end{itemize}

Almost all iterative methods need a good preconditioner to function properly.
We will use several ready-to-use preconditioners that have been implemented
in several packages in Julia such as incomplete LU and multigrid preconditioners.
In the next section we will describe a diagonalization method based
on nonlinear minimization as we apply it to a simple case of 2d harmonic
potential.


\section{Minimization approach: 2d harmonic potential}

We will test our implementation for solving Schroedinger equation for two
dimensional harmonic potentials:
%
\begin{equation}
V(x,y) = \frac{1}{2} \omega^2 (x^2 + y^2)
\end{equation}
%
This potential can be implemented in the following Julia code.
%
\begin{juliacode}
function pot_harmonic( grid::FD2dGrid; ω=1.0 )
  Npoints = grid.Npoints
  Vpot = zeros(Npoints)
  for i in 1:Npoints
    x = grid.r[1,i]
    y = grid.r[2,i]
    Vpot[i] = 0.5 * ω^2 *( x^2 + y^2 )
  end
  return Vpot
end
\end{juliacode}

This potential has the following analytic solutions for eigenvalues:
\begin{equation}
E_{n_{x} + n_{y}} = \hbar \omega \left( n_{x} + n_{y} + 1 \right)
\end{equation}
For each energy levels, we may have more than one possible combinations
of $n_x$ and $n_y$, for examples:

\begin{table}[h]
\centering
\begin{tabular}{|c|l|}
\hline
$E = n_x + n_y + 1$  &  Values of $n_x$ and $n_y$ \\
\hline
1                  &  (0,0) \\
2                  &  (1,0) (0,1) \\
3                  &  (2,0) (1,1) (1,1) \\
4                  &  (3,0) (0,3) (2,1) (1,2) \\
\hline
\end{tabular}
\end{table}

Our first task is to build the Hamiltonian. The step is very similar
to the one that we have done for 1d case:
\begin{juliacode}
Nx = 50
Ny = 50
grid = FD2dGrid( (-5.0,5.0), Nx, (-5.0,5.0), Ny )
∇2 = build_nabla2_matrix( grid )
Vpot = pot_harmonic( grid )
Ham = -0.5*∇2 + spdiagm( 0 => Vpot )
\end{juliacode}
Note that we have used \jlinline{spdiagm} instead of
\jlinline{diagm} when constructing the potential matrix.

\subsection{Orthonormalization}

We usually need to provide a guess solution or vectors to our eigensolver.
The guess vectors should be orthonarmalized properly before they can be supplied to
the eigensolver. There are several algorithms that can be used to orthonormalize
the vectors. One of them that will be used here is the Lowdin orthonormalization
which can be implemented in Julia as:
\begin{juliacode}
function ortho_sqrt( psi )
  Udagger = inv(sqrt(psi'*psi))
  return psi*Udagger
end
\end{juliacode}

As an example usage, here we start from random vectors and orthonormalize them
with \jlinline{ortho_sqrt}:
\begin{juliacode}
dVol = grid.dVol
Nstates = 3
Npoints = Nx*Ny
X = rand(Float64, Npoints, Nstates)
ortho_sqrt!(X, dVol)
\end{juliacode}

TODO: Gram-Schmidt ortho

TODO: check that the vectors are properly orthonormalized.


\subsection{Band energy functional and its gradient}

One method that can be used is diagonalize the Hamiltonian is based on the minimization
of band energy:
\begin{equation}
\min E\left[{\psi_{i}}\right] = \braket{\psi | H | \psi} - \sum_{ij}\lambda_{ij}\left(
\braket{ \psi_{i} | \psi_{j} } - \delta_{ij}
\right)
\end{equation}

The gradient of this expression is:
\begin{equation}
\frac{\partial H}{\partial \psi^{*}_{i}} = H\psi_{i} -
\sum_{j} \psi_{j} \braket{\psi_{j} | H | \psi_{i}}
\end{equation}
%
In Julia this can be implemented using the following function:
%
\begin{juliacode}
function calc_grad_evals!( Ham, ψ, g, Hsub )
  Nstates = size(ψ,2)
  Hψ = Ham*ψ
  Hsub[:] = ψ' * Hψ
  g[:,:] = Hψ - ψ*Hsub
  return
end
\end{juliacode}

\subsection{Steepest descent method}

The simplest method that we can apply to minimize the band energy functional is
the steepest descent method. This method can be described in the following steps:
\begin{enumerate}
\item Initialize random wave function: $\mathbf{X}$
\item Calculate $E$ for the given $\mathbf{X}$.
\item Calculate the gradient $\mathbf{g} = \nabla E$.
\item Set search direction $\mathbf{d} = -\mathbf{g}$.
\item Update wave function according to: $\mathbf{X} \leftarrow X + \alpha d$, where $\alpha$ is
a fixed step length. Orthonormalize $\mathbf{X}$.
\item Calculate $E$ for this updated $\mathbf{X}$ and compare it with the previous value.
\end{enumerate}

The following Julia code implements the steepest descent algorithm for minimizing band energy.
\begin{juliacode}
for iter = 1:NiterMax
  calc_grad_evals!( Ham, X, g, Hsub )
  d[:] = -g[:]
  # Update wavefunction
  X[:] = X + α_t*d
  ortho_sqrt!(X)
  Hr = Hermitian( X' * ( Ham*X ) )
  evals = eigvals(Hr)
  Ebands = sum(evals)
  devals = abs.( evals - evals_old )
  evals_old = copy(evals)
  nconv = length( findall( devals .< tol ) )
  diffE = abs(Ebands-Ebands_old)
  @printf("SD step %8d = %18.10f   %10.7e  nconv = %5d\n", iter, Ebands, diffE, nconv)
  if nconv >= Nstates_conv
      IS_CONVERGED = true
      @printf("Emin_SD convergence: nconv = %5d in %5d iterations\n", nconv, iter)
      break
  end
  Ebands_old = Ebands
end
\end{juliacode}

Note we have calculate the current estimate the eigenvalues and band energy in the following
lines:
\begin{juliacode}
Hr = Hermitian( X' * ( Ham*X ) )
evals = eigvals(Hr)
Ebands = sum(evals)
\end{juliacode}

In the file \txtinline{main_harmonic_Emin_SD.kl} we implement steepest descent algorithm
to find three lowest eigenstates of 2d harmonic potential.
Using the following parameters:
\begin{juliacode}
NiterMax = 5000
α = 3e-3
tol = 1e-6
\end{juliacode}
we obtain the results:
\begin{textcode}
evals[  1] =       0.9999999732 devals =   5.2402526762e-14
evals[  2] =       2.0000187702 devals =   1.1408453116e-07
evals[  3] =       2.0001656321 devals =   9.9907378681e-07  
\end{textcode}
which are close to the analytical solution. You might want to vary the number of eigenstates
that you want to search by setting higher values for the variable \jlinline{Nstates} to
higher than 3.

Despite its simplicity, the steepest descent method has several drawbacks.
The most prominent one is that it is very slow to converge. For this particular case
we have:
\begin{textcode}
SD step     2258 =       5.0001854888   1.1198655e-06  nconv =     2
SD step     2259 =       5.0001843756   1.1131584e-06  nconv =     3
Emin_SD convergence: nconv =     3 in  2259 iterations
\end{textcode}
The convergence is also dependent to the step length parameter \jlinline{α}.

\subsection{Line minimization}

Line minimization:
\begin{juliacode}
Xc = ortho_sqrt( X + α_t*d )
calc_grad_evals!( Ham, Xc, gt, Hsub )
denum = real(sum(conj(g-gt).*d))
if denum != 0.0
    α = abs( α_t*real(sum(conj(g).*d))/denum )
else
    α = 0.0
end
\end{juliacode}

Implemented in the program \txtinline{main_harmonic_Emin_linmin.jl}.
Result using line minimization:
\begin{textcode}
linmin step      589 =       5.0000566204   1.4525395e-06  nconv =     2
linmin step      590 =       5.0000552040   1.4164238e-06  nconv =     3
Emin_linmin convergence: nconv =     3 in   590 iterations
  
Eigenvalues:
  
evals[  1] =       1.0000001750 devals =   3.4402208904e-07
evals[  2] =       2.0000051631 devals =   1.0311948540e-07
evals[  3] =       2.0000498659 devals =   9.6928218651e-07  
\end{textcode}


\subsection{Preconditioning}

Action of preconditioning:

\begin{juliacode}
Kg[:] = g[:] # copy
for i in 1:Nstates
    @views ldiv!(prec, Kg[:,i])
end
\end{juliacode}

ILU preconditioner based on kinetic operator:
\begin{juliacode}
prec = ilu(-0.5*∇2)
\end{juliacode}

Result using ILU preconditioner based on kinetic operator:
\begin{textcode}
linmin step       55 =       5.0000009960   1.2258080e-06  nconv =     2
linmin step       56 =       5.0000003689   6.2711750e-07  nconv =     3
Emin_linmin convergence: nconv =     3 in    56 iterations

Eigenvalues:

evals[  1] =       0.9999999739 devals =   6.0261565960e-07
evals[  2] =       1.9999999937 devals =   5.8729066055e-09
evals[  3] =       2.0000004014 devals =   1.8628935727e-08  
\end{textcode}

ILU preconditioner based on Hamiltonian operator:
\begin{juliacode}
prec = ilu(Ham)
\end{juliacode}

\begin{textcode}
linmin step       15 =       5.0000004295   3.2233493e-06  nconv =     2
linmin step       16 =       4.9999998058   6.2374058e-07  nconv =     3
Emin_linmin convergence: nconv =     3 in    16 iterations
  
Eigenvalues:
  
evals[  1] =       0.9999999787 devals =   1.2492943446e-07
evals[  2] =       1.9999998478 devals =   1.2682084205e-07
evals[  3] =       1.9999999792 devals =   3.7199030434e-07
\end{textcode}


Multigrid preconditioner
\begin{juliacode}
prec = aspreconditioner(ruge_stuben(Ham))
\end{juliacode}

\begin{textcode}
linmin step       15 =       5.0000010097   4.8543236e-06  nconv =     2
linmin step       16 =       4.9999999515   1.0581623e-06  nconv =     3
Emin_linmin convergence: nconv =     3 in    16 iterations

Eigenvalues:

evals[  1] =       0.9999999796 devals =   1.0151917307e-07
evals[  2] =       1.9999998532 devals =   1.9803365658e-07
evals[  3] =       2.0000001187 devals =   7.5860946458e-07
\end{textcode}


ILU0 preconditioner, in SPARSKIT.
\begin{juliacode}
prec = ILU0Preconditioner(Ham)
\end{juliacode}

\begin{textcode}
linmin step       64 =       5.0000041335   1.4349680e-06  nconv =     2
linmin step       65 =       5.0000030469   1.0865516e-06  nconv =     3
Emin_linmin convergence: nconv =     3 in    65 iterations

Eigenvalues:

evals[  1] =       1.0000001632 devals =   1.3899380646e-07
evals[  2] =       2.0000000485 devals =   6.1344835878e-08
evals[  3] =       2.0000028351 devals =   8.8621293415e-07
\end{textcode}


For testing purpose
\begin{juliacode}
prec = NoPreconditioner()
\end{juliacode}

Comparison of preconditioner size
\begin{textcode}
sizeof Ham  =       0.6372146606 MiB
sizeof prec =       6.9084167480 MiB  ILU kinetic
sizeof prec =       3.0494079590 MiB  ILU Hamiltonian
sizeof prec =       2.3046264648 MiB  AMG Ruge-Stuben
sizeof prec =       0.6372070313 MiB  ILU0
\end{textcode}


\subsection{Conjugate gradient}

Conjugate-gradient

\begin{textcode}
d = -Kg + β * d_prev
\end{textcode}

Polak-Ribiere formula
\begin{juliacode}
if iter != 1
  β = real(sum(conj(g-g_prev).*Kg))/real(sum(conj(g_prev).*Kg_prev))
end
if β < 0.0 β = 0.0 end
\end{juliacode}

CG using ILU0 (Ham)
\begin{textcode}
CG step       29 =       5.0000026056   2.8129344e-06  nconv =     2
CG step       30 =       5.0000011500   1.4555620e-06  nconv =     3
Emin_CG convergence: nconv =     3 in    30 iterations
  
Eigenvalues:
  
evals[  1] =       1.0000001156 devals =   1.4606434040e-07
evals[  2] =       2.0000002046 devals =   6.6796002196e-07
evals[  3] =       2.0000008298 devals =   6.4153765811e-07  
\end{textcode}

\subsection{Eigenfunctions}

\begin{juliacode}
ortho_sqrt!(X)
Hr = Hermitian( X' * (Ham*X) )
evals, evecs = eigen(Hr)
X[:,:] = X*evecs
\end{juliacode}

The eigenfunctions are shown in Figure \ref{fig:harm_2d_eigenfunctions}.

\begin{figure}[h]
{\centering
\includegraphics[width=0.4\textwidth]{../codes/sch_2d/IMG_harmonic_psi_1.pdf}\\
\includegraphics[width=0.4\textwidth]{../codes/sch_2d/IMG_harmonic_psi_2.pdf}%
\includegraphics[width=0.4\textwidth]{../codes/sch_2d/IMG_harmonic_psi_3.pdf}
\par}
\caption{Visualization of eigenstates of 2d harmonic potential}
\label{fig:harm_2d_eigenfunctions}
\end{figure}

\section{Exercises}

2d Gaussian potential